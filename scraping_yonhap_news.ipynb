{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (366 of 366) |###############################################################| Elapsed Time: 0:12:57 Time: 0:12:57\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from  bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import progressbar\n",
    "\n",
    "\n",
    "title = []\n",
    "date = []\n",
    "link = []\n",
    "content = []\n",
    "\n",
    "\n",
    "mydatetime = datetime.date(2016,1,1)\n",
    "url_date = []\n",
    "for k in range(0,366):\n",
    "    date2016 = mydatetime+ datetime.timedelta(k)\n",
    "    date2016 = str(date2016)\n",
    "    date2016 = date2016.replace(\"-\", \"\")\n",
    "    url_date.append(date2016)\n",
    "\n",
    "def scraping(turl, title, date, link, content):\n",
    "    source_code = requests.get(turl)\n",
    "    plain_text = source_code.content\n",
    "    soup = BeautifulSoup(plain_text, 'lxml', from_encoding = 'utf-8')\n",
    "    body = soup.find('div', class_ = 'list_body newsflash_body')\n",
    "    yonhap = body.find('ul', class_ = 'type02')\n",
    "    \n",
    "    page_length = len(soup.find('div',class_='paging paging_ytn').find_all('a'))+2\n",
    "    \n",
    "    for x in range(1, page_length):\n",
    "        source_code = requests.get(turl+\"&page=\"+str(x))\n",
    "        plain_text = source_code.content\n",
    "        soup = BeautifulSoup(plain_text, 'lxml', from_encoding = 'utf-8')\n",
    "        body = soup.find('div', class_ = 'list_body newsflash_body')\n",
    "        yonhap = body.find('ul', class_ = 'type02')    \n",
    "        \n",
    "        for t in yonhap.find_all('strong'):\n",
    "            if (t.get_text() != '동영상기사'):\n",
    "                title.append(t.get_text())    \n",
    "        \n",
    "        for i in yonhap.find_all('span', class_ = 'date'):\n",
    "            date.append(i.get_text())\n",
    "        \n",
    "        for j in yonhap.find_all('a', class_ = 'nclicks(fls.list)', href=True):\n",
    "            link.append(j['href'])\n",
    "    \n",
    "with progressbar.ProgressBar(max_value = len(url_date)) as bar:\n",
    "    for x in url_date:\n",
    "        turl = \"http://news.naver.com/main/list.nhn?sid2=140&sid1=001&oid=001&isYeonhapFlash=Y&mid=sec&mode=LPOD&date=\"+str(x)\n",
    "        scraping(turl, title, date, link, content)\n",
    "        #print(url_date.index(x) , \"/\" ,len(url_date) )\n",
    "        bar.update(url_date.index(x))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data = pd.DataFrame({'title':title, 'date':date, 'link':link})\n",
    "def scrap_contents(data, start, finish):\n",
    "    with progressbar.ProgressBar(max_value=finish-start) as bar:\n",
    "        for i, x in enumerate(data.link[start:finish]):\n",
    "            url = x\n",
    "            source_code = requests.get(url)\n",
    "            plain_text = source_code.content\n",
    "            soup = BeautifulSoup(plain_text, 'lxml', from_encoding='utf-8')\n",
    "\n",
    "            article_content = soup.find('div', id=\"articleBodyContents\")\n",
    "            if (article_content != None):\n",
    "                try:\n",
    "                    for span in article_content.find_all('span'):\n",
    "                        article_content.span.extract()\n",
    "                except: pass\n",
    "                try:\n",
    "                    for script in article_content.find_all('script'):\n",
    "                        article_content.script.extract()\n",
    "                except: pass\n",
    "\n",
    "                for a in article_content.find_all('a'):\n",
    "                    article_content.a.extract()\n",
    "\n",
    "                try:\n",
    "                    article_text = article_content.get_text().split('=')[2]\n",
    "                    data.content[i+start] = article_text\n",
    "                except:\n",
    "                    data.content[i+start] = \"속보\"\n",
    "                    pass\n",
    "\n",
    "            article_content2 = soup.find('div', id=\"articeBody\")\n",
    "            if (article_content2 != None):\n",
    "                try:\n",
    "                    for span in article_content2.find_all('span'):\n",
    "                        article_content2.span.extract()\n",
    "                except: pass\n",
    "                try:\n",
    "                    for script in article_content2.find_all('script'):\n",
    "                        article_content2.script.extract()\n",
    "                except: pass\n",
    "\n",
    "                for a in article_content2.find_all('a'):\n",
    "                    article_content2.a.extract()\n",
    "\n",
    "                try:\n",
    "                    article_text = article_content2.get_text().split('=')[2]\n",
    "                    data.content[i+start] = article_text\n",
    "                except:\n",
    "                    data.content[i+start] = \"속보\"\n",
    "                    pass\n",
    "            article_content3 = soup.find('div', id=\"newsEndContents\")\n",
    "            if (article_content3 != None):\n",
    "                try:\n",
    "                    for span in article_content3.find_all('span'):\n",
    "                        article_content3.span.extract()\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    for script in article_content3.find_all('script'):\n",
    "                        article_content3.script.extract()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                for a in article_content3.find_all('a'):\n",
    "                    article_content3.a.extract()\n",
    "\n",
    "                try:\n",
    "                    article_text = article_content3.get_text().split('=')[2]\n",
    "                    data.content[i+start] = article_text\n",
    "                except:\n",
    "                    data.content[i+start] = \"속보\"\n",
    "                    pass\n",
    "\n",
    "            if (article_content == None and article_content2 == None and article_content3 == None):\n",
    "                print(x, \"예외 발생\")\n",
    "            time.sleep(1)\n",
    "            bar.update(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29000, 30000, 31000]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#range 지정\n",
    "a = range(29,32)\n",
    "b = [j * 1000 for j in a]\n",
    "#b[0] = 26890\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |#############################################################| Elapsed Time: 0:14:31 Time: 0:14:31\n",
      "N/A% (0 of 1000) |                                                               | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000 /31000 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |#############################################################| Elapsed Time: 0:04:08 Time: 0:04:08\n",
      "100% (1000 of 1000) |#############################################################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 /31000 완료\n",
      "31000 /31000 완료\n"
     ]
    }
   ],
   "source": [
    "for i in b:\n",
    "    scrap_contents(data, i, i+1000)\n",
    "    time.sleep(120)\n",
    "    print(i,\"/31000 완료\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     1,     2, ..., 29001, 29002, 29003], dtype=int64),)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#어디까지 했나 확인\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.where(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"data_complete.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
